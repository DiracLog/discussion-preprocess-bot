import json
import time
from huggingface_hub import hf_hub_download
from llama_cpp import Llama
import re
import textwrap


class StructureAnalyst:
    def __init__(self, repo_id="TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
                 filename="mistral-7b-instruct-v0.2.Q4_K_M.gguf"):
        print(f"‚è≥ Loading Analyst ({filename})...")
        model_path = hf_hub_download(repo_id=repo_id, filename=filename)
        self.context_limit = 5000

        self.llm = Llama(
            model_path=model_path,
            n_gpu_layers=-1,
            n_ctx=8192,
            verbose=True
        )
        print("‚úÖ Analyst Loaded.")

    @staticmethod
    def extract_json(txt):
        """
        Helper to clean markdown formatting from the LLM output.
        """

        pattern = r"```json(.*?)```"
        match = re.search(pattern, txt, re.DOTALL)

        if match:
            return match.group(1).strip()

        # find first outer { ... }
        pattern_fallback = r"\{.*\}"
        match_fallback = re.search(pattern_fallback, txt, re.DOTALL)

        if match_fallback:
            return match_fallback.group(0).strip()

        return txt

    def smart_summarize(self, full_text):
        """
        Decides whether to do a One-Shot or Map-Reduce summary based on length.
        """
        # token count ~ words/4
        estimated_tokens = len(full_text) / 4

        if estimated_tokens < self.context_limit:
            print("üü¢ Short text. Running standard analysis...")
            return self.extract_structure(full_text)
        else:
            print(f"üî¥ Long text ({int(estimated_tokens)} tokens). Engaging Map-Reduce...")
            return self.map_reduce_analysis(full_text)

    def map_reduce_analysis(self, text):
        chunks = []
        chunk_size = 15000
        overlap = 1000

        start = 0
        while start < len(text):
            end = min(start + chunk_size, len(text))
            chunks.append(text[start:end])
            if end == len(text): break
            start += chunk_size - overlap

        intermediate_summaries = []

        # analyze each chunk separately, to not lose context
        for i, chunk in enumerate(chunks):
            print(f"   üß† Processing Chunk {i + 1}/{len(chunks)}...")
            prompt = f"""[INST]
                        –ê–ù–ê–õ–Ü–ó –°–ï–ì–ú–ï–ù–¢–£ (Raw Data Extraction).
                        –¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äî –≤–∏—Ç—è–≥–Ω—É—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ —Ñ–∞–∫—Ç–∏.

                        1. –ó–Ω–∞–π–¥–∏ –≤—Å—ñ –∑–≥–∞–¥–∫–∏ –º–µ–¥—ñ–∞ (—Ñ—ñ–ª—å–º–∏, —ñ–≥—Ä–∏, –∫–Ω–∏–≥–∏). –ó–±–µ—Ä–µ–∂–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—É –Ω–∞–∑–≤—É.
                        2. –í–∏–ø–∏—à–∏ —Ü–∏—Ñ—Ä–æ–≤—ñ –æ—Ü—ñ–Ω–∫–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ "8 –∑ 10") –¥–æ—Å–ª—ñ–≤–Ω–æ.
                        3. –í–∏–ø–∏—à–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –∞—Ä–≥—É–º–µ–Ω—Ç–∏ (—á–æ–º—É —Å–ø–æ–¥–æ–±–∞–ª–æ—Å—å/–Ω–µ —Å–ø–æ–¥–æ–±–∞–ª–æ—Å—å).

                        –§–û–†–ú–ê–¢ –í–Ü–î–ü–û–í–Ü–î–Ü (–°–ø–∏—Å–æ–∫):
                        - –¢–≤—ñ—Ä: [–ù–∞–∑–≤–∞] | –û—Ü—ñ–Ω–∫–∞: [–ß–∏—Å–ª–æ/–§—Ä–∞–∑–∞] | –ê—Ä–≥—É–º–µ–Ω—Ç–∏: [–¢–µ–∑–∞ 1, –¢–µ–∑–∞ 2]
                        ...

                        –Ø–∫—â–æ —É —Ü—å–æ–º—É —à–º–∞—Ç–∫—É –Ω–µ–º–∞—î –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è —Ç–≤–æ—Ä—ñ–≤, –Ω–∞–ø–∏—à–∏ "–ü–£–°–¢–û".

                        –¢–ï–ö–°–¢ –°–ï–ì–ú–ï–ù–¢–£:
                        {chunk}
                        [/INST]"""

            response = self.llm(
                prompt,
                max_tokens=1024,
                temperature=0.1,
                stop=["</s>"],
                top_p=0.95,
                echo=False
            )
            text_result = response['choices'][0]['text']
            intermediate_summaries.append(text_result)

        # join chunks and summarise partitial summaries
        print("   üîó Combining summaries...")
        combined_text = "\n".join(intermediate_summaries)

        final_structure = self.extract_structure(combined_text, is_notes=True)

        return final_structure

    def extract_structure(self, transcription, is_notes=False):
        print(f"\nüß† Analyst is thinking... (Input length: {len(transcription)} chars)")

        input_description = "–í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç - —Ü–µ ¬´—Å–∏—Ä–∞¬ª —Å—Ç–µ–Ω–æ–≥—Ä–∞–º–∞ –∑ Whisper (ASR)."
        if is_notes:
            input_description = "–í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç - —Ü–µ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –∑—ñ–±—Ä–∞–Ω—ñ –Ω–æ—Ç–∞—Ç–∫–∏ (—Ñ–∞–∫—Ç–∏) –∑ –¥–æ–≤–≥–æ—ó —Ä–æ–∑–º–æ–≤–∏."

        # TODO add extracting from db from same user and update info
        system_prompt = f"""–¢–∏ - —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∏–π —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ç–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫ —Ä–æ–∑–º–æ–≤.
               {input_description}

               –¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è:
               1. –ó–Ω–∞–π—Ç–∏ –í–°–Ü –æ–±–≥–æ–≤–æ—Ä–µ–Ω—ñ —Ç–≤–æ—Ä–∏.
               2. –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–≤–æ—Ä—É —ñ –ö–û–ñ–ù–û–ì–û —Å–ø—ñ–∫–µ—Ä–∞ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –û–ö–†–ï–ú–ò–ô –∑–∞–ø–∏—Å. 
               3. –ù–ï –∑–º—ñ—à—É–≤–∞—Ç–∏ –¥—É–º–∫–∏ —Ä—ñ–∑–Ω–∏—Ö –ª—é–¥–µ–π. –Ø–∫—â–æ –ê–Ω–¥—Ä—ñ–π —ñ –û–ª–µ–∫—Å—ñ–π –≥–æ–≤–æ—Ä–∏–ª–∏ –ø—Ä–æ "–î—é–Ω—É", —Ü–µ –º–∞—î –±—É—Ç–∏ –î–í–ê —Ä—ñ–∑–Ω–∏—Ö –æ–±'—î–∫—Ç–∏ –≤ —Å–ø–∏—Å–∫—É.

               –§–æ—Ä–º–∞—Ç –≤–∏–≤–æ–¥—É: –¢–Ü–õ–¨–ö–ò –≤–∞–ª—ñ–¥–Ω–∏–π JSON –æ–±'—î–∫—Ç (–±–µ–∑ markdown –±–ª–æ–∫—ñ–≤ ```json):
               {{
                 "reviews": [
                   {{
                     "title": "–ù–∞–∑–≤–∞ —Ç–≤–æ—Ä—É",
                     "type": "book/movie/game/series",
                     "sentiment": "positive/negative/mixed",
                     "arguments": ["–ê—Ä–≥—É–º–µ–Ω—Ç 1", ...],
                     "mark": 8.5, 
                     "is_inferred_score": true,
                     "speaker": "–Ü–º'—è —Å–ø—ñ–∫–µ—Ä–∞ (–û–ë–û–í'–Ø–ó–ö–û–í–û)" 
                   }}
                 ]
               }}
               """

        user_prompt = f"–î–ê–ù–Ü –î–õ–Ø –ê–ù–ê–õ–Ü–ó–£:\n{transcription}"
        full_prompt = f"[INST] {system_prompt}\n\n{user_prompt} [/INST]"

        start_time = time.time()
        output = self.llm(
            full_prompt,
            max_tokens=4096,
            temperature=0.1,
            stop=["</s>"],
            echo=False
        )
        print(f"‚ö° LLM Inference complete in {time.time() - start_time:.2f} seconds.")

        raw_text = output['choices'][0]['text'].strip()
        clean_json_text = self.extract_json(raw_text)

        try:
            return json.loads(clean_json_text)
        except json.JSONDecodeError:
            print(f"‚ùå Model failed to generate valid JSON. Raw text:\n{raw_text}")
            return {"reviews": []}  # Safe fallback


if __name__ == "__main__":
    a = StructureAnalyst()
    print("Analyst ready.")