import json
import logging
import re
import time
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional

from huggingface_hub import hf_hub_download
from llama_cpp import Llama


# ---------------------- CONFIG ----------------------

@dataclass
class AnalystConfig:
    repo_id: str = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
    filename: str = "mistral-7b-instruct-v0.2.Q4_K_M.gguf"
    context_limit: int = 5000
    chunk_size: int = 15000
    overlap: int = 1000
    max_tokens_standard: int = 4096
    max_tokens_chunk: int = 1024
    temperature: float = 0.1
    n_ctx: int = 8192
    n_gpu_layers: int = -1


# ---------------------- MAIN CLASS ----------------------

class StructureAnalyst:
    def __init__(self, config: AnalystConfig = AnalystConfig()):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.setLevel(logging.INFO)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

        self.logger.info(f"Downloading/Loading model: {config.filename}")
        model_path = hf_hub_download(
            repo_id=config.repo_id,
            filename=config.filename
        )

        self.logger.info(f"Initializing Llama from: {model_path}")
        self.llm = Llama(
            model_path=model_path,
            n_gpu_layers=config.n_gpu_layers,
            n_ctx=config.n_ctx,
            verbose=False  # Set to True if C++ debug logs are needed
        )
        self.logger.info("‚úÖ Analyst Model loaded successfully.")

    # ---------------------- PUBLIC API ----------------------

    def smart_summarize(self, text: str) -> Dict[str, Any]:
        """
        Main entry point. Automatically decides between One-Shot or Map-Reduce.
        """
        if self._is_short(text):
            self.logger.info("üü¢ Text fits context. Running standard analysis...")
            return self._extract_structure(text)

        self.logger.info(f"üî¥ Long text detected. Engaging Map-Reduce...")
        return self._map_reduce_analysis(text)

    # Legacy alias for backward compatibility if needed
    def extract_structure(self, text: str) -> Dict[str, Any]:
        return self.smart_summarize(text)

    # ---------------------- INTERNAL LOGIC ----------------------

    def _is_short(self, text: str) -> bool:
        estimated_tokens = len(text) // 4
        return estimated_tokens < self.config.context_limit

    def _map_reduce_analysis(self, text: str) -> Dict[str, Any]:
        chunks = self._split_text(text)
        summaries = []

        for i, chunk in enumerate(chunks):
            self.logger.info(f"üß† Processing Chunk {i + 1}/{len(chunks)}...")
            prompt = self._build_chunk_prompt(chunk)
            response = self._generate(prompt, self.config.max_tokens_chunk)
            summaries.append(response)

        self.logger.info("üîó Combining chunk summaries...")
        combined_text = "\n".join(summaries)

        # Final Pass: Extract clean JSON from the intermediate notes
        return self._extract_structure(combined_text, is_notes=True)

    def _split_text(self, text: str) -> List[str]:
        chunks = []
        start = 0
        text_len = len(text)

        while start < text_len:
            end = min(start + self.config.chunk_size, text_len)
            chunks.append(text[start:end])
            if end == text_len:
                break
            start += self.config.chunk_size - self.config.overlap
        return chunks

    def _extract_structure(self, text: str, is_notes: bool = False) -> Dict[str, Any]:
        self.logger.info("üß† Generating final structured JSON...")
        prompt = self._build_main_prompt(text, is_notes)
        raw_output = self._generate(prompt, self.config.max_tokens_standard)

        clean_json = self._clean_json_string(raw_output)

        try:
            return json.loads(clean_json)
        except json.JSONDecodeError:
            self.logger.error(f"‚ùå Failed to parse JSON. Raw output:\n{raw_output}")
            # Return empty structure to prevent crash
            return {"reviews": []}

    # ---------------------- PROMPTS ----------------------

    def _build_chunk_prompt(self, chunk: str) -> str:
        return f"""[INST]
–ê–ù–ê–õ–Ü–ó –°–ï–ì–ú–ï–ù–¢–£ (Raw Data Extraction).
–¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äî –≤–∏—Ç—è–≥–Ω—É—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ —Ñ–∞–∫—Ç–∏.

1. –ó–Ω–∞–π–¥–∏ –≤—Å—ñ –∑–≥–∞–¥–∫–∏ –º–µ–¥—ñ–∞ (—Ñ—ñ–ª—å–º–∏, —ñ–≥—Ä–∏, –∫–Ω–∏–≥–∏). –ó–±–µ—Ä–µ–∂–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—É –Ω–∞–∑–≤—É.
2. –í–∏–ø–∏—à–∏ —Ü–∏—Ñ—Ä–æ–≤—ñ –æ—Ü—ñ–Ω–∫–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ "8 –∑ 10") –¥–æ—Å–ª—ñ–≤–Ω–æ.
3. –í–∏–ø–∏—à–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –∞—Ä–≥—É–º–µ–Ω—Ç–∏ (—á–æ–º—É —Å–ø–æ–¥–æ–±–∞–ª–æ—Å—å/–Ω–µ —Å–ø–æ–¥–æ–±–∞–ª–æ—Å—å) —ñ –•–¢–û —Ü–µ —Å–∫–∞–∑–∞–≤ (–°–ø—ñ–∫–µ—Ä).

–§–û–†–ú–ê–¢ –í–Ü–î–ü–û–í–Ü–î–Ü (–°–ø–∏—Å–æ–∫):
- –°–ø—ñ–∫–µ—Ä: [–Ü–º'—è] | –¢–≤—ñ—Ä: [–ù–∞–∑–≤–∞] | –û—Ü—ñ–Ω–∫–∞: [–ß–∏—Å–ª–æ/–§—Ä–∞–∑–∞] | –î—É–º–∫–∞: [–ê—Ä–≥—É–º–µ–Ω—Ç–∏]
...

–Ø–∫—â–æ —É —Ü—å–æ–º—É —à–º–∞—Ç–∫—É –Ω–µ–º–∞—î –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è —Ç–≤–æ—Ä—ñ–≤, –Ω–∞–ø–∏—à–∏ "–ü–£–°–¢–û".

–¢–ï–ö–°–¢ –°–ï–ì–ú–ï–ù–¢–£:
{chunk}
[/INST]"""

    def _build_main_prompt(self, text: str, is_notes: bool) -> str:
        input_desc = "–¶–µ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –∑—ñ–±—Ä–∞–Ω—ñ –Ω–æ—Ç–∞—Ç–∫–∏ (—Ñ–∞–∫—Ç–∏) –∑ –¥–æ–≤–≥–æ—ó —Ä–æ–∑–º–æ–≤–∏." if is_notes else "–¶–µ —Å–∏—Ä–∞ —Å—Ç–µ–Ω–æ–≥—Ä–∞–º–∞ (transcript)."

        return f"""[INST]
–¢–∏ - –∞–Ω–∞–ª—ñ—Ç–∏–∫ –∫–Ω–∏–∂–∫–æ–≤–æ–≥–æ –∫–ª—É–±—É.
{input_desc}

–¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è:
1. –ó–Ω–∞–π—Ç–∏ –í–°–Ü –æ–±–≥–æ–≤–æ—Ä–µ–Ω—ñ —Ç–≤–æ—Ä–∏.
2. –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–≤–æ—Ä—É —ñ –ö–û–ñ–ù–û–ì–û —Å–ø—ñ–∫–µ—Ä–∞ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –û–ö–†–ï–ú–ò–ô –∑–∞–ø–∏—Å. 
3. –ù–ï –∑–º—ñ—à—É–≤–∞—Ç–∏ –¥—É–º–∫–∏ —Ä—ñ–∑–Ω–∏—Ö –ª—é–¥–µ–π. –Ø–∫—â–æ –ê–Ω–¥—Ä—ñ–π —ñ –û–ª–µ–∫—Å—ñ–π –≥–æ–≤–æ—Ä–∏–ª–∏ –ø—Ä–æ "–î—é–Ω—É", —Ü–µ –º–∞—î –±—É—Ç–∏ –î–í–ê —Ä—ñ–∑–Ω–∏—Ö –æ–±'—î–∫—Ç–∏ –≤ —Å–ø–∏—Å–∫—É.

–§–æ—Ä–º–∞—Ç –≤–∏–≤–æ–¥—É: –¢–Ü–õ–¨–ö–ò –≤–∞–ª—ñ–¥–Ω–∏–π JSON –æ–±'—î–∫—Ç (–±–µ–∑ markdown –±–ª–æ–∫—ñ–≤ ```json):
{{
  "reviews": [
    {{
      "title": "–ù–∞–∑–≤–∞ —Ç–≤–æ—Ä—É",
      "type": "book/movie/game/series",
      "sentiment": "positive/negative/mixed",
      "arguments": ["–ê—Ä–≥—É–º–µ–Ω—Ç 1", ...],
      "mark": 8.5, 
      "is_inferred_score": true,
      "speaker": "–Ü–º'—è —Å–ø—ñ–∫–µ—Ä–∞ (–û–ë–û–í'–Ø–ó–ö–û–í–û)" 
    }}
  ]
}}

–¢–ï–ö–°–¢ –î–õ–Ø –ê–ù–ê–õ–Ü–ó–£:
{text}
[/INST]"""

    # ---------------------- LLM CORE ----------------------

    def _generate(self, prompt: str, max_tokens: int) -> str:
        start_time = time.time()

        output = self.llm(
            prompt,
            max_tokens=max_tokens,
            temperature=self.config.temperature,
            stop=["</s>"],
            echo=False
        )

        duration = time.time() - start_time
        self.logger.info(f"‚ö° Inference complete in {duration:.2f}s")

        return output['choices'][0]['text'].strip()

    # ---------------------- UTILS ----------------------

    @staticmethod
    def _clean_json_string(text: str) -> str:
        # 1. Regex for markdown code blocks
        json_block = re.search(r"```json\s*(.*?)```", text, re.DOTALL)
        if json_block:
            return json_block.group(1).strip()

        # 2. Fallback: Find outer braces
        first_brace = text.find("{")
        last_brace = text.rfind("}")

        if first_brace != -1 and last_brace != -1:
            return text[first_brace:last_brace + 1]

        return text


# ---------------------- ENTRY POINT ----------------------

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # Simple test
    analyst = StructureAnalyst()
    dummy_text = "[10:00] Andrii: –î—é–Ω–∞ - —Ü–µ —à–µ–¥–µ–≤—Ä, 10/10. Alex: –ù–µ –∑–≥–æ–¥–µ–Ω, –Ω—É–¥–Ω–æ, 5/10."

    result = analyst.smart_summarize(dummy_text)
    print(json.dumps(result, indent=2, ensure_ascii=False))