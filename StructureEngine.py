import json
import time
from huggingface_hub import hf_hub_download
from llama_cpp import Llama
import re
import textwrap


class StructureAnalyst:
    def __init__(self, repo_id="TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
                 filename="mistral-7b-instruct-v0.2.Q4_K_M.gguf"):
        print(f"‚è≥ Loading Analyst ({filename})...")
        model_path = hf_hub_download(repo_id=repo_id, filename=filename)
        self.context_limit = 5000

        self.llm = Llama(
            model_path=model_path,
            n_gpu_layers=-1,
            n_ctx=8192,
            verbose=True  # Keeps C++ logs enabled just in case
        )
        print("‚úÖ Analyst Loaded.")

    @staticmethod
    def extract_json(txt):
        # 1. Try to find a JSON block inside Markdown tags
        pattern = r"```json(.*?)```"
        match = re.search(pattern, txt, re.DOTALL)

        if match:
            return match.group(1).strip()

        # 2. Fallback: Look for the first outer curly braces { ... }
        # This saves you if the model forgets the Markdown tags entirely
        pattern_fallback = r"\{.*\}"
        match_fallback = re.search(pattern_fallback, txt, re.DOTALL)

        if match_fallback:
            return match_fallback.group(0).strip()

        return txt  # Return original if nothing found (will likely error in json.loads)

    def smart_summarize(self, full_text):
        """
        Decides whether to do a One-Shot or Map-Reduce summary
        based on length.
        """
        # 1. Estimate Token Count (Roughly 4 chars per token)
        estimated_tokens = len(full_text) / 4

        if estimated_tokens < self.context_limit:
            print("üü¢ Short text. Running standard analysis...")
            return self.extract_structure(full_text)
        else:
            print(f"üî¥ Long text ({int(estimated_tokens)} tokens). Engaging Map-Reduce...")
            return self.map_reduce_analysis(full_text)

    def map_reduce_analysis(self, text):
        # STEP 1: CHUNK IT
        # Split into chunks of ~15,000 characters (approx 4k tokens)
        chunks = []
        chunk_size = 15000
        overlap = 1000

        start = 0
        while start < len(text):
            end = min(start + chunk_size, len(text))
            chunks.append(text[start:end])
            if end == len(text): break
            start += chunk_size - overlap


        intermediate_summaries = []

        # STEP 2: MAP (Process each chunk)
        for i, chunk in enumerate(chunks):
            print(f"   üß† Processing Chunk {i + 1}/{len(chunks)}...")
            prompt = f"""[INST]
                        –ê–ù–ê–õ–Ü–ó –°–ï–ì–ú–ï–ù–¢–£ (Raw Data Extraction).
                        –¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äî –≤–∏—Ç—è–≥–Ω—É—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ —Ñ–∞–∫—Ç–∏.

                        1. –ó–Ω–∞–π–¥–∏ –≤—Å—ñ –∑–≥–∞–¥–∫–∏ –º–µ–¥—ñ–∞ (—Ñ—ñ–ª—å–º–∏, —ñ–≥—Ä–∏, –∫–Ω–∏–≥–∏). –ó–±–µ—Ä–µ–∂–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—É –Ω–∞–∑–≤—É.
                        2. –í–∏–ø–∏—à–∏ —Ü–∏—Ñ—Ä–æ–≤—ñ –æ—Ü—ñ–Ω–∫–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ "8 –∑ 10") –¥–æ—Å–ª—ñ–≤–Ω–æ.
                        3. –í–∏–ø–∏—à–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –∞—Ä–≥—É–º–µ–Ω—Ç–∏ (—á–æ–º—É —Å–ø–æ–¥–æ–±–∞–ª–æ—Å—å/–Ω–µ —Å–ø–æ–¥–æ–±–∞–ª–æ—Å—å).

                        –§–û–†–ú–ê–¢ –í–Ü–î–ü–û–í–Ü–î–Ü (–°–ø–∏—Å–æ–∫):
                        - –¢–≤—ñ—Ä: [–ù–∞–∑–≤–∞] | –û—Ü—ñ–Ω–∫–∞: [–ß–∏—Å–ª–æ/–§—Ä–∞–∑–∞] | –ê—Ä–≥—É–º–µ–Ω—Ç–∏: [–¢–µ–∑–∞ 1, –¢–µ–∑–∞ 2]
                        ...

                        –Ø–∫—â–æ —É —Ü—å–æ–º—É —à–º–∞—Ç–∫—É –Ω–µ–º–∞—î –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è —Ç–≤–æ—Ä—ñ–≤, –Ω–∞–ø–∏—à–∏ "–ü–£–°–¢–û".

                        –¢–ï–ö–°–¢ –°–ï–ì–ú–ï–ù–¢–£:
                        {chunk}
                        [/INST]"""
            # Call your LLM here (assuming self.llm is your model function)
            # –í–∏–∫–ª–∏–∫ –º–æ–¥–µ–ª—ñ
            response = self.llm(
                prompt,
                max_tokens=1024,
                temperature=0.1,
                stop=["</s>"],
                top_p=0.95,
                echo=False  # dont repeat prompt
            )
            text_result = response['choices'][0]['text']
            intermediate_summaries.append(text_result)

        # STEP 3: REDUCE (Combine)
        print("   üîó Combining summaries...")
        combined_text = "\n".join(intermediate_summaries)

        # Final Pass: Extract the clean JSON structure from the combined notes
        final_structure = self.extract_structure(combined_text, is_notes=True)

        return final_structure


    def extract_structure(self, transcription, is_notes=False):
        input_description = "–í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç - —Ü–µ ¬´—Å–∏—Ä–∞¬ª —Å—Ç–µ–Ω–æ–≥—Ä–∞–º–∞ –∑ Whisper (ASR)."
        if is_notes:
            input_description = "–í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç - —Ü–µ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –∑—ñ–±—Ä–∞–Ω—ñ –Ω–æ—Ç–∞—Ç–∫–∏ (—Ñ–∞–∫—Ç–∏) –∑ –¥–æ–≤–≥–æ—ó —Ä–æ–∑–º–æ–≤–∏."

        system_prompt = f"""–¢–∏ - —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∏–π —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ç–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫ —Ä–æ–∑–º–æ–≤.
                {input_description}

                –¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è:
                1. –°—Ñ–æ—Ä–º—É–≤–∞—Ç–∏ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π JSON –∑ —É—Å—ñ–º–∞ –æ–±–≥–æ–≤–æ—Ä–µ–Ω–∏–º–∏ —Ç–≤–æ—Ä–∞–º–∏.
                2. –Ø–∫—â–æ —Ü–µ –Ω–æ—Ç–∞—Ç–∫–∏, –æ–±'—î–¥–Ω–∞–π –¥—É–±–ª—ñ–∫–∞—Ç–∏ (—è–∫—â–æ –æ–¥–∏–Ω —Ç–≤—ñ—Ä –∑–≥–∞–¥—É—î—Ç—å—Å—è —É –∫—ñ–ª—å–∫–æ—Ö —à–º–∞—Ç–∫–∞—Ö).
                3. –í–ò–ü–†–ê–í–ò–¢–ò –Ω–∞–∑–≤–∏ —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –æ—Ü—ñ–Ω–∫–∏.

                –§–æ—Ä–º–∞—Ç –≤–∏–≤–æ–¥—É: –¢–Ü–õ–¨–ö–ò –≤–∞–ª—ñ–¥–Ω–∏–π JSON –æ–±'—î–∫—Ç (–±–µ–∑ markdown –±–ª–æ–∫—ñ–≤ ```json):
                {{
                  "reviews": [
                    {{
                      "title": "–ù–∞–∑–≤–∞ —Ç–≤–æ—Ä—É",
                      "type": "book/movie/game/series",
                      "sentiment": "positive/negative/mixed",
                      "arguments": ["–ê—Ä–≥—É–º–µ–Ω—Ç 1", ...],
                      "mark": 8.5, 
                      "is_inferred_score": true
                    }}
                  ]
                }}
                """

        user_prompt = f"–î–ê–ù–Ü –î–õ–Ø –ê–ù–ê–õ–Ü–ó–£:\n{transcription}"
        full_prompt = f"[INST] {system_prompt}\n\n{user_prompt} [/INST]"

        start_time = time.time()

        output = self.llm(
            full_prompt,
            max_tokens=4096,
            temperature=0.1,
            stop=["</s>"],
            echo=False
        )

        end_time = time.time()
        print(f"   ‚ö° LLM Inference complete in {end_time - start_time:.2f} seconds.")

        raw_text = output['choices'][0]['text'].strip()

        clean_json_text = self.extract_json(raw_text)

        try:
            data = json.loads(clean_json_text)
            return data
        except json.JSONDecodeError:
            print(f"‚ùå Model failed to generate valid JSON. Raw text:\n{raw_text}")
            return None


if __name__ == "__main__":
    a = StructureAnalyst()
    print("Analyst ready.")